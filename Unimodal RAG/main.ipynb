{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a8f1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_science_document = [\n",
    "    \n",
    "    \"Performing data science work is often an iterative process, where the data scientist needs to return to earlier steps if they run into challenges.\",\n",
    "\n",
    "    \"There are many ways to categorize the data science process, but it often includes: Data collection, Data exploration, Data modeling, Model evaluation, and Model deployment and monitoring.\",\n",
    "\n",
    "    \"Data Collection: Data collection and preprocessing involves gathering data from various sources (such as databases, APIs, and web scraping), then cleaning and transforming the data to prepare it for analysis. This step involves dealing with missing, inconsistent, or noisy data and converting it into a structured format. Depending on the organization, a team of data engineers may support this step; however, it is common for the data scientist to manage this process as well. This requires intimate knowledge of data sources and the ability to write SQL queries, database code, or custom tools such as web scrapers.\",\n",
    "\n",
    "    \"Data Exploration: Data exploration involves conducting exploratory data analysis (EDA) to better understand the data, detect anomalies, and identify relationships between variables. The key to this step is to look for correlations and understand the distribution of the data. This involves using descriptive statistics and visualization techniques to summarize the data and gain insights. A data scientist should be able to use summary statistics, create descriptive visualizations, or use reporting tools such as Power BI or Tableau.\",\n",
    "\n",
    "    \"Data Modeling: Using what was learned in the data exploration step, data modeling is the step when the data scientist builds predictive or descriptive models using machine learning and statistical techniques that identify patterns and relationships in the data. Here, the data scientist selects appropriate algorithms, trains models on historical data, and validates their performance.\",\n",
    "\n",
    "    \"Model Evaluation: Model evaluation and optimization involves assessing the performance of models using metrics such as accuracy, RMSE, precision, recall, AUC, or F1-score. Based on these evaluations, data scientists may refine models or try alternative algorithms to improve performance. Understanding the reasons behind model predictions is crucial for trust and alignment with domain knowledge. The data scientist must ensure the model solves the organizational/business goal and communicate findings to both technical and non-technical stakeholders.\",\n",
    "\n",
    "    \"Model Deployment and Monitoring: Model deployment and monitoring involves implementing models in real-world applications, monitoring performance, and maintaining them to ensure continued accuracy and relevance. Data scientists may work with data engineering teams or use tools such as containers to implement models. After deployment, they may develop dashboards to monitor model performance and alert stakeholders if performance goes outside expected ranges.\",\n",
    "\n",
    "    \"Data science is a profession that incorporates many data-related tasks, particularly those involving acquisition, preparation, and delivery of data. While modeling gets most of the attention, the majority of the work (around 80%) often comes from data preparation, exploration, and operational tasks. This does not include other responsibilities like stakeholder communication, gathering requirements, debugging software, emails, and research.\",\n",
    "\n",
    "    \"Now that we understand the common tasks associated with the job, we can explore the different types (or flavors) of data science.\",\n",
    "\n",
    "    \"Dissecting the Flavors of Data Science\",\n",
    "\n",
    "    \"The role of a data scientist often covers many different skills. Data scientists are frequently asked to perform tasks such as designing database tables, programming ML algorithms, understanding statistics, and creating visuals to explain findings. However, it is difficult for one person to master every skill area.\",\n",
    "\n",
    "    \"Therefore, many data scientists become particularly skilled in one or two areas and maintain basic competencies in others. Their talents can be considered T-shaped: broad proficiency across many areas (horizontal line of the T), with deep expertise in a few areas (vertical line of the T).\",\n",
    "\n",
    "    \"A data scientist’s competencies are often aligned with their unique experiences or interests. For example, a statistics major may naturally excel in ML, while a former BI engineer with strong ETL experience may quickly grasp data engineering concepts.\",\n",
    "\n",
    "    \"It is important to master the fundamentals, but most people will develop their own T of Competencies — a combination of top skill sets that shapes their identity in the data science space.\",\n",
    "\n",
    "    \"Some of the most common flavors of data science include: Data Engineer, Dashboarding and Visual Specialist, ML Specialist, and Domain Expert.\",\n",
    "\n",
    "    \"Data Engineer: Data engineering is a crucial aspect of the data science process involving data collection, storage, processing, and management. It focuses on designing, developing, and maintaining scalable data infrastructure and ensuring the availability of high-quality data for analysis and modeling. Data engineers are most known for managing ETL pipelines and data workflows.\",\n",
    "\n",
    "    \"In smaller organizations, data engineering responsibilities may fall under the data science team. A data scientist specializing in this area supports projects by collecting and storing data, and structuring it so it can be efficiently fed into ML or deep learning algorithms.\",\n",
    "\n",
    "    \"Common tools for Data Engineers include: Programming languages (Python, SQL, Scala, R, C++), Data storage systems (MySQL, PostgreSQL, Oracle, MongoDB, Cassandra, DynamoDB, Snowflake, Redshift, BigQuery, HDFS), Data processing tools (Spark, Flink, Storm, Beam, MapReduce, Hadoop, Hive, Kafka, Kinesis), ETL tools (NiFi, Talend, Airflow, AWS Glue, Google Cloud Dataflow, dbt), Version control (Git, GitHub, GitLab, Bitbucket, Azure DevOps), Visualization tools (Tableau, Power BI, Looker, QlikView, Domo), Cloud platforms (Azure, GCP, AWS), and containerization tools (Docker, Kubernetes).\",\n",
    "\n",
    "    \"Dashboarding and Visual Specialist: Data visualization is the graphical representation of data using charts, graphs, and maps. It helps stakeholders understand complex patterns, trends, and relationships, supports decision-making, and communicates insights effectively. Strong visualization combined with storytelling can drive organizational action, and many news organizations hire data scientists specifically for visualization skills.\",\n",
    "\n",
    "    \"Dashboarding and visual specialists may be called BI engineers, data analysts, visualization experts, or data storytellers. They typically have strong skills in descriptive statistics, storytelling, and KPI development.\",\n",
    "\n",
    "    \"Common tools for Dashboarding and Visual Specialists include: Programming languages (Python, SQL, R, JavaScript), Data storage systems (MySQL, PostgreSQL, Oracle, MongoDB, Cassandra, DynamoDB, Snowflake, Redshift, BigQuery), Frameworks (Dask, Plotly, ggplot2, Shiny, Matplotlib, Seaborn, D3.js), BI tools (Tableau, Power BI, Looker, QlikView, Domo, Funnel, Excel), and cloud platforms (Azure, GCP, AWS).\",\n",
    "\n",
    "    \"ML Specialist: ML specialists focus on designing and implementing machine learning algorithms. They build models that allow computers to learn from experience without explicit programming. Their work involves analyzing data, identifying patterns, and making predictions or decisions. They are skilled in selecting appropriate algorithms and tuning parameters to achieve the best results.\",\n",
    "\n",
    "    \"ML specialists often stay current through research and are experienced in model development, deployment, and maintenance. Many have strong backgrounds in statistics, operations research, computer science, or information systems.\",\n",
    "\n",
    "    \"Common tools for ML Specialists include: Programming languages (Python, SQL, R, Java, C++), ML frameworks (TensorFlow, Keras, scikit-learn, PyTorch, H2O, Hugging Face), Data storage systems (MySQL, PostgreSQL, Oracle, MongoDB, Cassandra, DynamoDB, Snowflake, Redshift, BigQuery, HDFS), Processing tools (Spark, Flink, Storm, Beam, MapReduce, Kafka), ETL tools (NiFi, Talend, Airflow, AWS Glue, Google Cloud Dataflow), Version control (Git, GitHub, GitLab, Bitbucket), Cloud platforms (Azure, GCP, AWS), and deployment tools (Docker, Kubernetes, Flask).\",\n",
    "\n",
    "    \"Domain Expert: Domain experts are data scientists with deep knowledge in a specific domain, either technical (such as computer vision or natural language processing) or business-related (such as marketing, aviation, finance, healthcare, etc.). They use their domain expertise to build customized models and analysis methods tailored to domain-specific problems.\",\n",
    "\n",
    "    \"Non-technical domain experts may have an advantage in specialized data science roles because they understand industry challenges and workflows. For example, a digital marketing professional may excel in attribution modeling, while someone with aviation experience may excel in route optimization problems.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54571223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 162\n",
      "Performing data science work is often an iterative process, where the data scien\n",
      "where the data scientist needs to return to earlier steps if they run into chall\n",
      " they run into challenges.\n",
      "There are many ways to categorize the data science process, but it often include\n",
      "but it often includes: Data collection, Data exploration, Data modeling, Model e\n",
      "ta modeling, Model evaluation, and Model deployment and monitoring.\n",
      "toring.\n",
      "Data Collection: Data collection and preprocessing involves gathering data from \n",
      "gathering data from various sources (such as databases, APIs, and web scraping),\n",
      ", and web scraping), then cleaning and transforming the data to prepare it for a\n",
      " to prepare it for analysis. This step involves dealing with missing, inconsiste\n",
      " missing, inconsistent, or noisy data and converting it into a structured format\n",
      " a structured format. Depending on the organization, a team of data engineers ma\n",
      "of data engineers may support this step; however, it is common for the data scie\n",
      "on for the data scientist to manage this process as well. This requires intimate\n",
      "is requires intimate knowledge of data sources and the ability to write SQL quer\n",
      "ty to write SQL queries, database code, or custom tools such as web scrapers.\n",
      " as web scrapers.\n",
      "Data Exploration: Data exploration involves conducting exploratory data analysis\n",
      "ratory data analysis (EDA) to better understand the data, detect anomalies, and \n",
      "tect anomalies, and identify relationships between variables. The key to this st\n",
      ". The key to this step is to look for correlations and understand the distributi\n",
      "stand the distribution of the data. This involves using descriptive statistics a\n",
      "riptive statistics and visualization techniques to summarize the data and gain i\n",
      " the data and gain insights. A data scientist should be able to use summary stat\n",
      " to use summary statistics, create descriptive visualizations, or use reporting \n",
      "s, or use reporting tools such as Power BI or Tableau.\n",
      "Data Modeling: Using what was learned in the data exploration step, data modelin\n",
      "n step, data modeling is the step when the data scientist builds predictive or d\n",
      "ilds predictive or descriptive models using machine learning and statistical tec\n",
      " and statistical techniques that identify patterns and relationships in the data\n",
      "ionships in the data. Here, the data scientist selects appropriate algorithms, t\n",
      "priate algorithms, trains models on historical data, and validates their perform\n",
      "idates their performance.\n",
      "Model Evaluation: Model evaluation and optimization involves assessing the perfo\n",
      " assessing the performance of models using metrics such as accuracy, RMSE, preci\n",
      "ccuracy, RMSE, precision, recall, AUC, or F1-score. Based on these evaluations, \n",
      " these evaluations, data scientists may refine models or try alternative algorit\n",
      " alternative algorithms to improve performance. Understanding the reasons behind\n",
      "g the reasons behind model predictions is crucial for trust and alignment with d\n",
      "and alignment with domain knowledge. The data scientist must ensure the model so\n",
      " ensure the model solves the organizational/business goal and communicate findin\n",
      "d communicate findings to both technical and non-technical stakeholders.\n",
      "takeholders.\n",
      "Model Deployment and Monitoring: Model deployment and monitoring involves implem\n",
      "ring involves implementing models in real-world applications, monitoring perform\n",
      ", monitoring performance, and maintaining them to ensure continued accuracy and \n",
      "tinued accuracy and relevance. Data scientists may work with data engineering te\n",
      " data engineering teams or use tools such as containers to implement models. Aft\n",
      "mplement models. After deployment, they may develop dashboards to monitor model \n",
      "ds to monitor model performance and alert stakeholders if performance goes outsi\n",
      "rformance goes outside expected ranges.\n",
      "Data science is a profession that incorporates many data-related tasks, particul\n",
      "ated tasks, particularly those involving acquisition, preparation, and delivery \n",
      "ation, and delivery of data. While modeling gets most of the attention, the majo\n",
      " attention, the majority of the work (around 80%) often comes from data preparat\n",
      "s from data preparation, exploration, and operational tasks. This does not inclu\n",
      " This does not include other responsibilities like stakeholder communication, ga\n",
      "er communication, gathering requirements, debugging software, emails, and resear\n",
      ", emails, and research.\n",
      "Now that we understand the common tasks associated with the job, we can explore \n",
      "job, we can explore the different types (or flavors) of data science.\n",
      " science.\n",
      "Dissecting the Flavors of Data Science\n",
      "The role of a data scientist often covers many different skills. Data scientists\n",
      "lls. Data scientists are frequently asked to perform tasks such as designing dat\n",
      "uch as designing database tables, programming ML algorithms, understanding stati\n",
      " understanding statistics, and creating visuals to explain findings. However, it\n",
      "indings. However, it is difficult for one person to master every skill area.\n",
      "very skill area.\n",
      "Therefore, many data scientists become particularly skilled in one or two areas \n",
      "in one or two areas and maintain basic competencies in others. Their talents can\n",
      "s. Their talents can be considered T-shaped: broad proficiency across many areas\n",
      "cy across many areas (horizontal line of the T), with deep expertise in a few ar\n",
      "xpertise in a few areas (vertical line of the T).\n",
      "A data scientist’s competencies are often aligned with their unique experiences \n",
      " unique experiences or interests. For example, a statistics major may naturally \n",
      "major may naturally excel in ML, while a former BI engineer with strong ETL expe\n",
      "with strong ETL experience may quickly grasp data engineering concepts.\n",
      "g concepts.\n",
      "It is important to master the fundamentals, but most people will develop their o\n",
      "will develop their own T of Competencies — a combination of top skill sets that \n",
      "top skill sets that shapes their identity in the data science space.\n",
      "e space.\n",
      "Some of the most common flavors of data science include: Data Engineer, Dashboar\n",
      "a Engineer, Dashboarding and Visual Specialist, ML Specialist, and Domain Expert\n",
      "t, and Domain Expert.\n",
      "Data Engineer: Data engineering is a crucial aspect of the data science process \n",
      "ata science process involving data collection, storage, processing, and manageme\n",
      "essing, and management. It focuses on designing, developing, and maintaining sca\n",
      " and maintaining scalable data infrastructure and ensuring the availability of h\n",
      "he availability of high-quality data for analysis and modeling. Data engineers a\n",
      "ng. Data engineers are most known for managing ETL pipelines and data workflows.\n",
      " and data workflows.\n",
      "In smaller organizations, data engineering responsibilities may fall under the d\n",
      "may fall under the data science team. A data scientist specializing in this area\n",
      "alizing in this area supports projects by collecting and storing data, and struc\n",
      "ring data, and structuring it so it can be efficiently fed into ML or deep learn\n",
      "nto ML or deep learning algorithms.\n",
      "Common tools for Data Engineers include: Programming languages (Python, SQL, Sca\n",
      "es (Python, SQL, Scala, R, C++), Data storage systems (MySQL, PostgreSQL, Oracle\n",
      ", PostgreSQL, Oracle, MongoDB, Cassandra, DynamoDB, Snowflake, Redshift, BigQuer\n",
      "e, Redshift, BigQuery, HDFS), Data processing tools (Spark, Flink, Storm, Beam, \n",
      "Flink, Storm, Beam, MapReduce, Hadoop, Hive, Kafka, Kinesis), ETL tools (NiFi, T\n",
      ", ETL tools (NiFi, Talend, Airflow, AWS Glue, Google Cloud Dataflow, dbt), Versi\n",
      "ataflow, dbt), Version control (Git, GitHub, GitLab, Bitbucket, Azure DevOps), V\n",
      "et, Azure DevOps), Visualization tools (Tableau, Power BI, Looker, QlikView, Dom\n",
      "ooker, QlikView, Domo), Cloud platforms (Azure, GCP, AWS), and containerization \n",
      "nd containerization tools (Docker, Kubernetes).\n",
      "Dashboarding and Visual Specialist: Data visualization is the graphical represen\n",
      "e graphical representation of data using charts, graphs, and maps. It helps stak\n",
      " maps. It helps stakeholders understand complex patterns, trends, and relationsh\n",
      "ends, and relationships, supports decision-making, and communicates insights eff\n",
      "nicates insights effectively. Strong visualization combined with storytelling ca\n",
      "with storytelling can drive organizational action, and many news organizations h\n",
      "news organizations hire data scientists specifically for visualization skills.\n",
      "ualization skills.\n",
      "Dashboarding and visual specialists may be called BI engineers, data analysts, v\n",
      "rs, data analysts, visualization experts, or data storytellers. They typically h\n",
      "rs. They typically have strong skills in descriptive statistics, storytelling, a\n",
      "ics, storytelling, and KPI development.\n",
      "Common tools for Dashboarding and Visual Specialists include: Programming langua\n",
      ": Programming languages (Python, SQL, R, JavaScript), Data storage systems (MySQ\n",
      "torage systems (MySQL, PostgreSQL, Oracle, MongoDB, Cassandra, DynamoDB, Snowfla\n",
      "a, DynamoDB, Snowflake, Redshift, BigQuery), Frameworks (Dask, Plotly, ggplot2, \n",
      "k, Plotly, ggplot2, Shiny, Matplotlib, Seaborn, D3.js), BI tools (Tableau, Power\n",
      "ools (Tableau, Power BI, Looker, QlikView, Domo, Funnel, Excel), and cloud platf\n",
      "el), and cloud platforms (Azure, GCP, AWS).\n",
      "ML Specialist: ML specialists focus on designing and implementing machine learni\n",
      "nting machine learning algorithms. They build models that allow computers to lea\n",
      "low computers to learn from experience without explicit programming. Their work \n",
      "ramming. Their work involves analyzing data, identifying patterns, and making pr\n",
      "terns, and making predictions or decisions. They are skilled in selecting approp\n",
      " in selecting appropriate algorithms and tuning parameters to achieve the best r\n",
      "o achieve the best results.\n",
      "ML specialists often stay current through research and are experienced in model \n",
      "xperienced in model development, deployment, and maintenance. Many have strong b\n",
      ". Many have strong backgrounds in statistics, operations research, computer scie\n",
      "earch, computer science, or information systems.\n",
      "Common tools for ML Specialists include: Programming languages (Python, SQL, R, \n",
      "es (Python, SQL, R, Java, C++), ML frameworks (TensorFlow, Keras, scikit-learn, \n",
      "eras, scikit-learn, PyTorch, H2O, Hugging Face), Data storage systems (MySQL, Po\n",
      "e systems (MySQL, PostgreSQL, Oracle, MongoDB, Cassandra, DynamoDB, Snowflake, R\n",
      "namoDB, Snowflake, Redshift, BigQuery, HDFS), Processing tools (Spark, Flink, St\n",
      "ls (Spark, Flink, Storm, Beam, MapReduce, Kafka), ETL tools (NiFi, Talend, Airfl\n",
      "(NiFi, Talend, Airflow, AWS Glue, Google Cloud Dataflow), Version control (Git, \n",
      "rsion control (Git, GitHub, GitLab, Bitbucket), Cloud platforms (Azure, GCP, AWS\n",
      "rms (Azure, GCP, AWS), and deployment tools (Docker, Kubernetes, Flask).\n",
      "tes, Flask).\n",
      "Domain Expert: Domain experts are data scientists with deep knowledge in a speci\n",
      "knowledge in a specific domain, either technical (such as computer vision or nat\n",
      "mputer vision or natural language processing) or business-related (such as marke\n",
      "lated (such as marketing, aviation, finance, healthcare, etc.). They use their d\n",
      ".). They use their domain expertise to build customized models and analysis meth\n",
      "ls and analysis methods tailored to domain-specific problems.\n",
      ".\n",
      "Non-technical domain experts may have an advantage in specialized data science r\n",
      "lized data science roles because they understand industry challenges and workflo\n",
      "allenges and workflows. For example, a digital marketing professional may excel \n",
      "fessional may excel in attribution modeling, while someone with aviation experie\n",
      "ith aviation experience may excel in route optimization problems.\n",
      "lems.\n"
     ]
    }
   ],
   "source": [
    "def chunk_text (text, chunk_size = 80, overlap = 20):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "for doc in data_science_document:\n",
    "    chunks.extend(chunk_text(doc))\n",
    "print('Total chunks:', len(chunks))\n",
    "for c in chunks:\n",
    "    print(c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cdd32b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\win10\\Desktop\\foder\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 541.46it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedder.encode(chunks)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ebb9ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector stored 162\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss \n",
    "\n",
    "dimemssion = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimemssion)\n",
    "index.add(np.array(embeddings))\n",
    "print(\"vector stored\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3dc2dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved context:\n",
      "- Data Engineer: Data engineering is a crucial aspect of the data science process \n",
      "- Data science is a profession that incorporates many data-related tasks, particul\n"
     ]
    }
   ],
   "source": [
    "# Step 3: query\n",
    "query = \"what is Data Engineer ?\"\n",
    "query_embedding = embedder.encode([query]).astype(\"float32\")\n",
    "\n",
    "distances, indices = index.search(query_embedding, k=2)\n",
    "\n",
    "retrieved_chunks = [chunks[i] for i in indices[0]]\n",
    "\n",
    "print(\"\\nRetrieved context:\")\n",
    "for chunk in retrieved_chunks:\n",
    "    print(\"-\", chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3df6fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 190/190 [00:00<00:00, 409.79it/s, Materializing param=shared.weight]                                                       \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Data engineer is a profession that incorporates many data-related tasks, particul\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "context = \"\".join(retrieved_chunks)\n",
    "prompt = f\"\"\"\n",
    "Answer the question using only the context below\n",
    "\n",
    "Context\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "outputs = model.generate(**inputs, max_new_tokens=4000)\n",
    "answer =tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print (\"Answer:\" , answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
